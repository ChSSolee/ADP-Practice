# BERT (Bidirectional Encoder Representations from Transformers)

- 트랜스포머(transformer)의 등장 이후, 다양한 자연어 처리 태스크에서 사용되었던 RNN 계열의 신경망인 LSTM, GRU는 트랜스포머로 대체되어가는 추세
- 이에 따라 다양한 트랜스포머 계열의 BERT, GPT, T5 등 다양한 사전 훈련된 언어 모델들이 계속해서 등장

<br>

## NLP에서의 사전 훈련(Pre-training)

> "사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다."

- BERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 계열의 모델들이 자연어 처리를 지배
- BERT를 배우기에 앞서 워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리가 발전되어온 흐름에 대한 정리 필요

<br>

### 사전 훈련된 워드 임베딩

- **어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 존재**
1. 임베딩 층(Embedding layer)을 랜덤 초기화하여 처음부터 학습하는 방법
2. 방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 사전에 학습된 임베딩 벡터들을 가져와 사용하는 방법

<br>

- 만약, 태스크에 사용하기 위한 데이터가 적다면, 사전 훈련된 임베딩을 사용하면 성능 향상을 기대가능

- 이 두 가지 방법 모두 하나의 단어가 하나의 벡터값으로 맵핑되므로,
  
  문맥을 고려하지 못 하여 다의어나 동음이의어를 구분하지 못하는 문제점이 존재
  
  - 한국어에는 '사과'라는 단어가 존재하는데 이 '사과'는 용서를 빈다는 의미로도 쓰이지만, 먹는 과일의 의미로도 사용
  - 그러나 임베딩 벡터는 '사과'라는 벡터에 하나의 벡터값을 맵핑하므로 이 두 가지 의미를 구분할 수 없었음
  
  $\rightarrow$ **이 한계는 사전 훈련된 언어 모델을 사용하므로서 극복할 수 있었으며 ELMo나 BERT 등이 이러한 문제의 해결책**

<br>

### 사전 훈련된 언어 모델

<img scr='https://wikidocs.net/images/page/108730/image1.PNG'>

- 2015년 구글은 'Semi-supervised Sequence Learning'라는 논문에서 LSTM 언어 모델을 학습하고나서, 학습한 LSTM을 텍스트 분류에 추가 학습하는 방법을 진행
1. 우선 LSTM 언어 모델을 학습
- LSTM 언어 모델은 주어진 텍스트로부터 이전 단어들로부터 다음 단어를 예측하도록 학습하므로, 기본적으로 별도의 레이블이 부착되지 않은 텍스트 데이터로도 학습 가능
- 사전 훈련된 워드 임베딩과 마찬가지로 사전 훈련된 언어 모델의 강점은 학습 전 사람이 별도 레이블을 지정해줄 필요가 없다는 점
2. 이렇게 레이블이 없는 데이터로 학습된 LSTM과, 가중치가 랜덤으로 초기화 된 LSTM 두 가지를 두고, 텍스트 분류와 같은 문제를 학습하여

  사전 훈련된 언어 모델을 사용한 전자의 경우가 더 좋은 성능을 얻을 수 있다는 가능성을 확인

- **방대한 텍스트로 LSTM 언어 모델을 학습해두고, 언어 모델을 다른 태스크에서 높은 성능을 얻기 위해 사용하는 방법으로 ELMo와 같은 아이디어도 존재**

<br>

#### ELMO

<img src='https://wikidocs.net/images/page/108730/image2.PNG'>

- **ELMo는 순방향 언어 모델과 역방향 언어 모델을 각각 따로 학습시킨 후에,**
  
  **사전 학습된 언어 모델로부터 임베딩 값을 얻는다는 아이디어**
- 이러한 임베딩은 문맥에 따라서 임베딩 벡터값이 달라지므로,
  
  기존 워드 임베딩인 Word2Vec이나 GloVe 등이 다의어를 구분할 수 없었던 문제점을 해결
  
  $\rightarrow$ 언어 모델은 RNN 계열의 신경망에서 탈피

**트랜스포머가 번역기와 같은 인코더-디코더 구조에서 LSTM을 뛰어넘는 좋은 성능을 얻자, LSTM이 아닌 트랜스포머로 사전 훈련된 언어 모델을 학습하는 시도가 등장**

<br>

<img src='https://wikidocs.net/images/page/108730/image3.PNG'>

- 그림에서 Trm은 트랜스포머(Transformer)의 약자
- 트랜스포머의 디코더는 LSTM 언어 모델처럼 순차적으로 이전 단어들로부터 다음 단어를 예측

<br>

- Open AI는 트랜스포머 디코더로 총 12개의 층을 쌓은 후에 방대한 텍스트 데이터를 학습시킨 언어 모델 GPT-1을 생성
- Open AI는 GPT-1에 여러 다양한 태스크를 위해 추가 학습을 진행하였을 때, 다양한 태스크에서 높은 성능을 얻을 수 있음을 입증
- **NLP의 주요 트렌드는 사전 훈련된 언어 모델을 만들고 이를 특정 태스크에 추가 학습시켜 해당 태스크에서 높은 성능을 얻는 것으로 접어들었고,**

  **언어 모델의 학습 방법에 변화를 주는 모델들이 등장**


<br>

<img src='https://wikidocs.net/images/page/108730/image4.PNG'>

- 위의 좌측 그림에 있는 단방향 언어모델은 지금까지 배운 전형적인 언어 모델
  - 시작 토큰 \<SOS\>가 들어가면, 다음 단어 I를 예측하고, 그리고 그 다음 단어 am을 예측
- 반면, 우측에 있는 양방향 언어 모델은 지금까지 본 적 없던 형태의 언어 모델
  - 가령, 양방향 LSTM을 이용해서 우측과 같은 언어 모델을 만들었다고 했을 때.
  
    초록색 LSTM 셀은 순방향 언어 모델로 \<sos\>를 입력받아 'I'를 예측하고, 그 후에 'am'을 예측
    
    그런데 'am'을 예측할 때, 출력층은 주황색 LSTM 셀인 역방향 언어 모델의 정보도 함께 받고있음
    
    'am'을 예측하는 시점에서 역방향 언어 모델이 이미 관측한 단어는 'a', 'am', 'I' 이렇게 3개의 단어
    
    **이미 예측해야하는 단어를 역방향 언어 모델을 통해 미리 관측한 셈이므로, 언어 모델은 일반적으로 양방향으로 구현하지 않음**
  
- **하지만 언어의 문맥이라는 것은 실제로는 양방향**
  
  텍스트 분류나 개체명 인식 등에서 양방향 LSTM을 사용하여 모델을 구현해서 좋은 성능을 얻을 수 있었음
  
  **하지만 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로 인해 위의 그림과 같은 양방향 언어 모델을 사용할 수 없으므로,**
  
  **그 대안으로 ELMo에서는 순방향과 역방향이라는 두 개의 단방향 언어 모델을 따로 준비하여 학습하는 방법을 사용**

<br>

### 마스크드 언어 모델 (Masked Language Model)
- 기존 언어 모델로는 양방향 구조를 도입할 수 없으므로, 양방향 구조를 도입하기 위해서 2018년에는 새로운 구조의 언어 모델이 탄생

  $\rightarrow$ **마스크드 언어 모델**

- **마스크드 언어 모델은 입력 텍스트 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)**
  
  (마스킹이란 원래의 단어가 무엇이었는지 모르게 한다는 뜻)
  
  $\rightarrow$ **인공 신경망에게 이렇게 마스킹 된 단어들을(Masked words) 예측하도록 지시**
  
  $\rightarrow$ **문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 방식**
  
  - 예를 들어 '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'를 주고 [MASK]에 들어갈 단어를 맞추게 함.



<br>

<hr>

<br>

##  버트(Bidirectional Encoder Representations from Transformers, BERT)
- BERT(Bidirectional Encoder Representations from Transformers)는 2018년에 구글이 공개한 사전 훈련된 모델
  
  (BERT라는 이름은 세서미 스트리트라는 미국 인형극의 케릭터 이름이기도 한데, 앞서 소개한 임베딩 방법론인 ELMo와 마찬가지로 세서미 스트리트의 케릭터 이름을 따온 것)

- **BERT는 2018년에 공개되어 등장과 동시에 수많은 NLP 태스크에서 최고 성능을 보여주면서 명실공히 NLP의 한 획을 그은 모델**



<br>

- `transformers` 패키지 설치

```python
pip install transformers
```

<br>

## BERT의 개요

<img src='https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG'>

- **BERT는 트랜스포머를 이용하여 구현된, 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델**

- **BERT가 높은 성능을 얻을 수 있었던 것은, 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고,**

  **레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여,**
  
  **이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문**
  
  **다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 파인 튜닝(Fine-tuning)**

- 위의 그림은 BERT의 파인 튜닝 사례
- 태스크가 스팸 메일 분류라고 하였을 때, 이미 위키피디아 등으로 사전 학습된 BERT 위에 분류를 위한 신경망을 한 층 추가
  
  $\rightarrow$ 이 경우, BERT가 언어 모델 사전 학습 과정에서 얻은 지식을 활용할 수 있으므로 스팸 메일 분류에서 보다 더 좋은 성능

- 이전에 언급한 ELMo나 OpenAI GPT-1 등이 이러한 파인 튜닝 사례의 대표적인 사례


<br>

### BERT의 크기

<img src='https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG'>

- **BERT의 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조**
  - Base 버전에서는 총 12개를 쌓았으며, Large 버전에서는 총 24개를 쌓음
  - Large 버전은 Base 버전보다 d_model의 크기나 셀프 어텐션 헤드(Self Attention Heads)의 수가 더 큼
  - 트랜스포머 인코더 층의 수를 $L$, d_model의 크기를 $D$, 셀프 어텐션 헤드의 수를 $A$라고 하였을 때 각각의 크기는

> BERT-Base : $L=12$, $D=768$, $A=12$ : 110M개의 파라미터
>
> BERT-Large : $L=24$, $D=1024$, $A=16$ : 340M개의 파라미터


- 초기 트랜스포머 모델이 $L=6$, $D=512$, $A=8$이었다는 것과 비교하면,

  Base 또한 초기 트랜스포머보다는 큰 네트워크
- BERT-base는 BERT보다 앞서 등장한 Open AI GPT-1과 하이퍼파라미터가 동일

  이는 BERT 연구진이 직접적으로 GPT-1과 성능을 비교하기 위해서 GPT-1과 동등한 크기로 BERT-Base를 설계하였기 때문
  
- BERT-Large는 BERT의 최대 성능을 보여주기 위해 만들어진 모델
  - BERT가 세운 기록들은 대부분 BERT-Large를 통해 달성




<br>

### BERT의 문맥을 반영한 임베딩(Contextual Embedding)
- **BERT는 ELMo나 GPT-1과 마찬가지로 문맥을 반영한 임베딩(Contextual Embedding)을 사용**

<img src='https://wikidocs.net/images/page/115055/bert0.PNG'>

- **BERT의 입력은 딥 러닝 모델들과 마찬가지로 임베딩 층(Embedding layer)를 지난 임베딩 벡터들**
- d_model을 768로 정의하였으므로, 모든 단어들은 768차원의 임베딩 벡터가 되어 BERT의 입력으로 사용
  
  $\rightarrow$ BERT는 내부적인 연산을 거친 후, 동일하게 각 단어에 대해서 768차원의 벡터를 출력

- 위의 그림에서는 BERT가 각 768차원의 [CLS], 'I', 'love', 'you'라는 4개의 벡터를 입력받아서(입력 임베딩) 동일하게 768차원의 4개의 벡터를 출력하는 모습(출력 임베딩)

<br>

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG'>

- **BERT의 연산을 거친 후의 출력 임베딩은, 문장의 문맥을 모두 참고한, 문맥을 반영한 임베딩**
- **위의 좌측 그림에서 [CLS]라는 벡터는, BERT의 초기 입력으로 사용되었을 입력 임베딩 당시에는, 단순히 임베딩 층(embedding layer)를 지난 임베딩 벡터였지만,**
  
  **BERT를 지나고 나서는 [CLS],'I', 'love', 'you'라는 모든 단어 벡터들을 모두 참고한 후에 문맥 정보를 가진 벡터가 됨**
  
  (위의 좌측 그림에서는 모든 단어를 참고하고 있다는 것을 점선의 화살표로 표현)

- 이는 [CLS]라는 단어 벡터 뿐만 아니라 다른 벡터들도 전부 마찬가지
  - 가령, 우측의 그림에서 출력 임베딩 단계의 'love'를 보면 BERT의 입력이었던 모든 단어들인 [CLS], 'I', 'love', 'you'를 참고

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC3.PNG'>

- **하나의 단어가 모든 단어를 참고하는 연산은 BERT의 12개의 층에서 전부 이루어지는 연산**
  
  12개의 층을 지난 후에 최종적으로 출력 임베딩을 얻게됨,
- 위의 그림은 BERT의 첫번째 층에 입력된 각 단어가 모든 단어를 참고한 후에 출력되는 과정을 화살표로 표현
- **BERT의 첫번째 층의 출력 임베딩은 BERT의 두번째 층에서는 입력 임베딩**

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG'>

- **BERT는 '셀프 어텐션'을 통하여, 모든 단어들을 참고한 문맥을 반영한 출력 임베딩을 얻게됨**
- **BERT는 기본적으로 트랜스포머 인코더를 12번 쌓은 것이므로 내부적으로 각 층마다 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드 포워드 신경망을 수행**




<br>

### BERT의 서브워드 토크나이저 : WordPiece
- **BERT는 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저를 사용**
- BERT가 사용한 토크나이저는 WordPiece 토크나이저로, 바이트 페어 인코딩(Byte Pair Encoding, BPE)의 유사 알고리즘
  - 동작 방식은 BPE와 조금 다르지만, 글자로부터 서브워드들을 병합해가는 방식으로 최종 단어 집합(Vocabulary)을 만드는 것은 BPE와 유사

- **서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만,**
  
  **자주 등장하지 않는 단어의 경우에는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어**
  
- 단어 집합이 만들어지고 나면, 이 단어 집합을 기반으로 토큰화를 수행
- 서브워드 토크나이저 패키지인 `SentencePiece`과 같이 BERT의 서브워드 토크나이저도 동작

<br>

#### BERT에서 토큰화를 수행하는 방식
- 준비물 : 이미 훈련 데이터로부터 만들어진 단어 집합
1. 토큰이 단어 집합에 존재
  
  $\rightarrow$ 해당 토큰을 분리하지 않음

2. 토큰이 단어 집합에 존재하지 않음
  
  $\rightarrow$ 해당 토큰을 서브워드로 분리
  
  $\rightarrow$ 해당 토큰의 첫번째 서브워드를 제외한 나머지 서브워드들은, 앞에 "##"를 붙인 것을 토큰으로 함

<br>

- **예를 들어 'embeddings'이라는 단어가 입력으로 들어왔을 때, BERT의 단어 집합에 해당 단어가 존재하지 않았다면**
- **만약, 서브워드 토크나이저가 아닌 토크나이저라면 여기서 OOV 문제가 발생,**
  
  **하지만 서브워드 토크나이저의 경우에는 해당 단어가 단어 집합에 존재하지 않았다고 해서, 서브워드 또한 존재하지 않는다는 의미는 아니므로,**
    
  $\rightarrow$ **해당 단어를 더 쪼개려고 시도**
    
  $\rightarrow$ **만약, BERT의 단어 집합에 'em', '##bed', '##ding', '#s'라는 서브 워드들이 존재한다면, 'embeddings'는 'em', '##bed', '##ding', '#s'로 분리**
    
  $\rightarrow$ **여기서 '##'은 이 서브워드들은 단어의 중간부터 등장하는 서브워드라는 것을 알려주기 위해 단어 집합 생성 시 표시해둔 기호**
    
  - 이런 표시가 있어야만 em, ##bed, ##ding, #s를 다시 손쉽게 embeddings로 복원


```python
import pandas as pd
from transformers import BertTokenizer
```

    /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: 
    The secret `HF_TOKEN` does not exist in your Colab secrets.
    To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
    You will be able to reuse this secret in all of your notebooks.
    Please note that authentication is recommended but still optional to access public models or datasets.
      warnings.warn(
    


    tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]



    vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]



    tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]



    config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]


<br>

#### `BertTokenizer.from_pretrained()` : BERT 토크나이저 객체


```python
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased") # Bert-base의 토크나이저
```


```python
result = tokenizer.tokenize('Here is the sentence I want embeddings for.')
print(result)
```

    ['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']
    

<br>

#### `BERT토크나이저.vocab()` : BERT의 단어 집합에 특정 단어가 있는지 조회

- 'here'이 정수 인코딩을 위해서 단어 집합 내부적으로 2182라는 정수로 맵핑되어져 있다는 의미


```python
print(tokenizer.vocab['here'])
```

    2182
    

- 'embeddings' 단어가 존재하지 않는다는 의미


```python
print(tokenizer.vocab['embeddings'])
```


    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    <ipython-input-6-4ef7a948935c> in <cell line: 1>()
    ----> 1 print(tokenizer.vocab['embeddings'])
    

    KeyError: 'embeddings'


* **하지만 단어 em, ##bed, ##ing, ##s는 모두 단어 집합에 존재**


```python
print(tokenizer.vocab['em'])
```

    7861
    


```python
print(tokenizer.vocab['##bed'])
```

    8270
    


```python
print(tokenizer.vocab['##ding'])
```

    4667
    


```python
print(tokenizer.vocab['##s'])
```

    2015
    

<br>

- BERT의 단어 집합 전체를 로드 : BERT의 단어 집합을 vocabulary.txt에 저장


```python
with open('vocabulary.txt', 'w') as f:
  for token in tokenizer.vocab.keys():
    f.write(token + '\n')
```


```python
df = pd.read_fwf('vocabulary.txt', header=None)
df.head()
```





  <div id="df-fbfe1955-58e0-48c4-bd0e-3574a8ab3be2" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[PAD]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[unused0]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[unused1]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[unused2]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[unused3]</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-fbfe1955-58e0-48c4-bd0e-3574a8ab3be2')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-fbfe1955-58e0-48c4-bd0e-3574a8ab3be2 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-fbfe1955-58e0-48c4-bd0e-3574a8ab3be2');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-6e8d4720-c36b-4612-9ed5-a1ae7c1bb4c5">
  <button class="colab-df-quickchart" onclick="quickchart('df-6e8d4720-c36b-4612-9ed5-a1ae7c1bb4c5')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-6e8d4720-c36b-4612-9ed5-a1ae7c1bb4c5 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>





```python
print('단어 집합의 크기 :',len(df))
```

    단어 집합의 크기 : 30522
    

- 정수로부터 맵핑된 단어를 확인


```python
df.loc[4667].values[0]
```




    '##ding'




```python
df.loc[102].values[0]
```




    '[SEP]'



<br>

#### BERT에서 사용되는 특별 토큰들과 그와 맵핑되는 정수
- [PAD] - 0
- [UNK] - 100
* [CLS] - 101
* [SEP] - 102
* [MASK] - 103



```python
df.loc[102].values[0]
```




    '[SEP]'



<br>

### 포지션 임베딩(Position Embedding)
- 트랜스포머에서는 포지셔널 인코딩(Positional Encoding)이라는 방법을 통해서 단어의 위치 정보를 표현
  - 포지셔널 인코딩은 사인 함수와 코사인 함수를 사용하여 위치에 따라, 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법
- **BERT에서는 이와 유사하지만, 위치 정보를 사인 함수와 코사인 함수로 만드는 것이 아닌,**
  
  **학습을 통해서 얻는 포지션 임베딩(Position Embedding)이라는 방법을 사용**

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG'>

- 위의 그림은 포지션 임베딩을 사용하는 방법
- 위의 그림에서 WordPiece Embedding은 우리가 이미 알고 있는 단어 임베딩으로 실질적인 입력
  
  이 입력에 포지션 임베딩을 통해서 위치 정보를 더해줌
  
  **여기서 포지션 임베딩의 아이디어는 위치 정보를 위한 임베딩 층(Embedding layer)을 하나 더 사용하는 것**
  
  - 문장의 길이가 4라면, 4개의 포지션 임베딩 벡터를 학습. 그리고 BERT의 입력마다 다음과 같이 포지션 임베딩 벡터를 더해줌

> 첫번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터
>
> 두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터
>
> 세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터
>
> 네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터

<br>

- **실제 BERT에서는 문장의 최대 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습**
- **결론적으로 현재 설명한 내용을 기준으로는 BERT에서는 총 두 개의 임베딩 층이 사용**
- 단어 집합의 크기가 30,522개인 단어 벡터를 위한 임베딩 층과 문장의 최대 길이가 512이므로, 512개의 포지션 벡터를 위한 임베딩 층

* **추가적으로 BERT는 세그먼트 임베딩(Segment Embedding)이라는 1개의 임베딩 층을 더 사용**



<br>

### BERT의 사전 훈련 (Pre-training)

<img src='https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png'>

- 위의 그림은 BERT의 논문에 첨부된 그림으로 ELMo와 GPT-1, 그리고 BERT의 구조적인 차이를 보여줌
- **가장 우측 그림의 ELMo는 정방향 LSTM과 역방향 LSTM을 각각 훈련시키는 방식으로 양방향 언어 모델을 생성**
- **가운데 그림의 GPT-1은 트랜스포머의 디코더를 이전 단어들로부터 다음 단어를 예측하는 방식으로 단방향 언어 모델을 생성**
  - Trm은 트랜스포머를 의미

- **단방향(→)으로 설계된 Open AI GPT와 달리 가장 좌측 그림의 BERT는 화살표가 양방향으로 뻗어나가는 모습**
  
  **이는 마스크드 언어 모델(Masked Language Model)을 통해 양방향성을 얻었기 때문**

- **BERT의 사전 훈련 방법은 크게 두 가지**
1. 마스크드 언어 모델
2. 다음 문장 예측(Next sentence prediction, NSP)

- 논문에 따르면 BERT는 BookCorpus(8억 단어)와 위키피디아(25억 단어)로 학습


<br>

#### 1) 마스크드 언어 모델(Masked Language Model, MLM)
- **BERT는 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 15%의 단어를 랜덤으로 마스킹(Masking)**
  
  $\rightarrow$ **그리고 인공 신경망에게 이 가려진 단어들을(Masked words) 예측하도록 설정**
  
  $\rightarrow$ **중간에 단어들에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 방식**

  - 예를 들어 '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'를 주고 '슈퍼'와 '우유'를 맞추게 함

<br>

- **더 정확히는 전부 [MASK]로 변경하지는 않고, 랜덤으로 선택된 15%의 단어들은 다시 다음과 같은 비율로 규칙이 적용**
  - **80%의 단어들은 [MASK]로 변경**
  
    Ex) The man went to the store → The man went to the [MASK]

  - **10%의 단어들은 랜덤으로 단어가 변경**

    Ex) The man went to the store → The man went to the dog

  - **10%의 단어들은 동일**

    Ex) The man went to the store → The man went to the store

- **이렇게 하는 이유는 [MASK]만 사용할 경우에는, [MASK] 토큰이 파인 튜닝 단계에서는 나타나지 않으므로**
  
  **사전 학습 단계와 파인 튜닝 단계에서의 불일치가 발생하는 문제가 발생하기 때문**
  
  $→$ **이 문제을 완화하기 위해서 랜덤으로 선택된 15%의 단어들의 모든 토큰을 [MASK]로 사용하지 않음**
  
<br>

#### 마스킹 비율

<img src='https://wikidocs.net/images/page/115055/%EC%A0%84%EC%B2%B4%EB%8B%A8%EC%96%B4.PNG'>

- 전체 단어의 85%는 마스크드 언어 모델의 학습에 사용되지 않음
- **마스크드 언어 모델의 학습에 사용되는 단어는 전체 단어의 15%**
- 학습에 사용되는 12%는 [MASK]로 변경 후에 원래 단어를 예측
- 1.5%는 랜덤으로 단어가 변경된 후에 원래 단어를 예측
- 1.5%는 단어가 변경되지는 않았지만, BERT는 이 단어가 변경된 단어인지 원래 단어가 맞는지는 알 수 없음
  
  $\rightarrow$ 이 경우에도 BERT는 원래 단어가 무엇인지를 예측

<br>

#### 마스크드 언어 모델 과정

- 'My dog is cute. he likes playing'이라는 문장에 대해서 마스크드 언어 모델을 학습
  
  $→$ 약간의 전처리와 BERT의 서브워드 토크나이저에 의해 이 문장은 ['my', 'dog', 'is' 'cute', 'he', 'likes', 'play', '##ing']로 토큰화
  
  $→$ BERT의 입력으로 사용
  
  $\rightarrow$ 언어 모델 학습을 위해서 다음과 같이 데이터가 변경되었다고 가정

> 'dog' 토큰은 [MASK]로 변경

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC8.PNG'>

- 위 그림은 'dog' 토큰이 [MASK]로 변경되어서 BERT 모델이 원래 단어를 맞추려고 하는 모습
- **여기서 출력층에 있는 다른 위치의 벡터들은 예측과 학습에 사용되지 않고,**
  
  **오직 'dog' 위치의 출력층의 벡터만이 사용**
  
  - **구체적으로는 BERT의 손실 함수에서 다른 위치에서의 예측은 무시**
- **출력층에서는 예측을 위해 단어 집합의 크기만큼의 밀집층(Dense layer)에 소프트맥스 함수가 사용된 1개의 층을 사용하여 원래 단어가 무엇인지를 맞추게 됨**

<br>

> 'dog' 토큰은 [MASK]로 변경
>
> 'he'는 랜덤 단어 'king'으로 변경
>
> 'play'는 변경되진 않았지만 예측에 사용

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG'>

- **BERT는 랜덤 단어 'king'으로 변경된 토큰에 대해서도 원래 단어가 무엇인지, 변경되지 않은 단어 'play'에 대해서도 원래 단어가 무엇인지를 예측**
- **'play'는 변경되지 않았지만, BERT 입장에서는 이것이 변경된 단어인지 아닌지 모르므로 마찬가지로 원래 단어를 예측**






<br>

#### 2) 다음 문장 예측(Next Sentence Prediction, NSP)
- **BERT는 마스크드 언어 모델 외에도 다음 문장 예측이라는 또 다른 태스크를 학습**
- **BERT는 두 개의 문장을 준 후에 이 문장이 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련**
  
  $→$ 이를 위해서 50:50 비율로 실제 이어지는 두 개의 문장과, 랜덤으로 이어붙인 두 개의 문장을 주고 훈련
  
  $\rightarrow$ 이를 각각 Sentence A와 Sentence B라고 하였을 때

> - 이어지는 문장의 경우
> > Sentence A : The man went to the store.
> >
> > Sentence B : He bought a gallon of milk.
> >
> > Label = IsNextSentence
>
> - 이어지는 문장이 아닌 경우
> > Sentence A : The man went to the store.
> >
> > Sentence B : dogs are so cute.
> >
> > Label = NotNextSentence

<br>

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG'>

- **BERT의 입력으로 넣을 때에는 [SEP]라는 특별 토큰을 사용해서 문장을 구분**
  - 첫번째 문장의 끝에 [SEP] 토큰을 넣고, 두번째 문장이 끝나면 역시 [SEP] 토큰을 붙임
- **그리고 이 두 문장이 실제 이어지는 문장인지 아닌지를, [CLS] 토큰의 위치의 출력층에서 이진 분류 문제로 연결**
  - [CLS] 토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰
- **그리고 위의 그림에서 나타난 것과 같이 마스크드 언어 모델과 다음 문장 예측은 따로 학습하는 것이 아닌, loss를 합하여 학습이 동시에 이루어짐**

<br>

- **BERT가 언어 모델 외에도 다음 문장 예측이라는 태스크를 학습하는 이유는**
  
  **BERT가 풀고자 하는 태스크 중에서는 QA(Question Answering)나 NLI(Natural Language Inference)와 같이 두 문장의 관계를 이해하는 것이 중요한 태스크들이 존재하기 때문**




<br>

### 세그먼트 임베딩(Segment Embedding)

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG'>

- BERT는 QA 등과 같은 두 개의 문장 입력이 필요한 태스크에 사용
- **문장 구분을 위해서 BERT는 세그먼트 임베딩이라는 또 다른 임베딩 층(Embedding layer)을 사용**
- **첫번째 문장에는 Sentence 0 임베딩, 두번째 문장에는 Sentence 1 임베딩을 더해주는 방식이며, 임베딩 벡터는 두 개만 사용**

<br>

#### 결론적으로 BERT는 총 3개의 임베딩 층이 사용
1. WordPiece Embedding : 실질적인 입력이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기로 30,522개.
2. Position Embedding : 위치 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.
3. Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.

<br>

#### BERT에 여러 문장이 입력되는 경우
- **많은 문헌에서 BERT가 문장 중간의 [SEP] 토큰과 두 종류의 세그먼트 임베딩을 통해서 두 개의 문장을 구분하여 입력받을 수 있다고 설명하고 있지만,**
  
  **BERT에 두 개의 문장이 들어간다는 표현에서 문장이라는 것은 실제 우리가 알고 있는 문장의 단위가 아님**
  
- **예를 들어 QA 문제를 푸는 경우에는 [SEP]와 세그먼트 임베딩을 기준으로 구분되는 [질문(Question), 본문(Paragraph)] 두 종류의 텍스트를 입력받지만,**
    
  **본문(Paragraph) 1개는 실제로는 다수의 문장으로 구성될수 있음**
    
  $\rightarrow$ **[SEP]과 세그먼트 임베딩으로 구분되는 BERT의 입력에서, 두 개의 문장은 실제로는 두 종류의 텍스트, 두 개의 문서일 수 있음**

<br>

#### BERT가 두 개의 문장을 입력받을 필요가 없는 경우
- **IMDB 리뷰 분류와 같은 감성 분류 태스크에서**

  **한 개의 문서에 대해서만 분류를 하는 것이므로, 이 경우에는 BERT의 전체 입력에 Sentence 0 임베딩만을 더해줌**



<br>

### BERT 파인 튜닝 (Fine-Tuning)
- **사전 학습 된 BERT에 우리가 풀고자 하는 태스크의 데이터를 추가로 학습 시켜서 테스트하는 단계**
- 실질적으로 태스크에 BERT를 사용하는 단계

<br>

#### 1) 하나의 텍스트에 대한 텍스트 분류 유형(Single Text Classification)

<img src='https://wikidocs.net/images/page/115055/apply1.PNG'>

- **영화 리뷰 감성 분류, 로이터 뉴스 분류 등과 같이 입력된 문서에 대해서 분류를 하는 유형**
- **문서의 시작에 [CLS] 라는 토큰을 입력**
  - [CLS] 토큰은 BERT가 분류 문제를 풀기위한 특별 토큰이며, 이는 BERT를 실질적으로 사용하는 단계인 파인 튜닝 단계에서도 동일
- **텍스트 분류 문제를 풀기 위해서 [CLS] 토큰의 위치의 출력층에서 밀집층(Dense layer) 또는 같은 이름으로는 완전 연결층(fully-connected layer)이라고 불리는 층들을 추가하여 분류에 대한 예측**


<br>

#### 2) 하나의 텍스트에 대한 태깅 작업(Tagging)

<img src='https://wikidocs.net/images/page/115055/apply2.PNG'>

- 대표적으로 문장의 각 단어에 품사를 태깅하는 품사 태깅 작업과 개체를 태깅하는 개체명 인식 작업
- **출력층에서 입력 텍스트의 각 토큰의 위치에 밀집층을 사용하여 분류에 대한 예측**


<br>

#### 3) 텍스트의 쌍에 대한 분류 또는 회귀 문제(Text Pair Classification or Regression)

<img src='https://wikidocs.net/images/page/115055/apply3.PNG'>

- **텍스트의 쌍을 입력으로 받는 대표적인 태스크로 자연어 추론(Natural language inference)**
  - **자연어 추론 문제 : 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는지를 분류**
  - 유형으로는 모순 관계(contradiction), 함의 관계(entailment), 중립 관계(neutral)

- **텍스트의 쌍을 입력받는 이러한 태스크의 경우에는 입력 텍스트가 1개가 아니므로, 텍스트 사이에 [SEP] 토큰을 집어넣고,**
  
  $\rightarrow$ **Sentence 0 임베딩과 Sentence 1 임베딩이라는 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분**


<br>

#### 4) 질의 응답(Question Answering)

<img src='https://wikidocs.net/images/page/115055/apply4.PNG'>

- 텍스트의 쌍을 입력으로 받는 또 다른 태스크로 QA(Question Answering)
- **BERT로 QA를 풀기 위해서 '질문'과 '본문'이라는 두 개의 텍스트의 쌍을 입력**
  - 이 태스크의 대표적인 데이터셋으로 SQuAD(Stanford Question Answering Dataset) v1.1
- **질문과 본문을 입력받으면, 본문의 일부분을 추출해서 질문에 답변**

  Q : "강우가 떨어지도록 영향을 주는 것은?"
  
  A : "기상학에서 강우는 대기 수증기가 응결되어 `중력`의 영향을 받고 떨어지는 것을 의미합니다. 강우의 주요 형태는 이슬비, 비, 진눈깨비, 눈, 싸락눈 및 우박이 있습니다."

  **라는 본문이 주어졌을 때 $→$ 정답은 "중력"**


<br>

### 어텐션 마스크 (Attention Mask)

<img src='https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC11.PNG'>

- **BERT를 실제로 실습하게 되면 어텐션 마스크라는 시퀀스 입력이 추가로 필요**
- **어텐션 마스크는 BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록,**
  
  **실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력**
  
- **0과 1 두 가지 값을 가짐**
  
  - 숫자 1 : 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미
  
  - 숫자 0 : 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미
  
- 위의 그림과 같이 실제 단어의 위치에는 1, 패딩 토큰의 위치에는 0의 값을 가지는 시퀀스를 만들어 BERT의 또 다른 입력으로 사용

