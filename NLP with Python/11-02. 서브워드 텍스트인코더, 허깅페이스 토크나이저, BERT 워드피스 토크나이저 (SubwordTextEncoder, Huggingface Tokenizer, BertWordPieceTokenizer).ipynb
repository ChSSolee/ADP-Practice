{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP5N55Tcl/2R/SqhZpxFR7+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 서브워드텍스트인코더(SubwordTextEncoder)\n","* SubwordTextEncoder는 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저\n","- BPE와 유사한 알고리즘인 Wordpiece Model을 채택하였으며, 패키지를 통해 쉽게 단어들을 서브워드들로 분리가능\n","\n","```\n","Tensorflow 2.3+ 버전에서는 tfds.features.text 대신 tfds.deprecated.text라고 작성\n","```"],"metadata":{"id":"SNzF8ksIYv28"}},{"cell_type":"markdown","source":["<br>\n","\n","### IMDB 리뷰 토큰화"],"metadata":{"id":"E-MVZKKUZONB"}},{"cell_type":"code","source":["import pandas as pd\n","import urllib.request\n","import tensorflow_datasets as tfds"],"metadata":{"id":"C--DejNiZRH_","executionInfo":{"status":"ok","timestamp":1687962422760,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n","\n","train_df = pd.read_csv('IMDb_Reviews.csv')"],"metadata":{"id":"XO6Ygv-RZRdn","executionInfo":{"status":"ok","timestamp":1687962423390,"user_tz":-540,"elapsed":632,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### `tensorflow_datasets.deprecated.text.SubwordTextEncoder.build_from_corpus(토큰화할 데이터, target_vocab_size)` :  서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여"],"metadata":{"id":"FyBeB4PzZVWD"}},{"cell_type":"code","source":["tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size=2**13)"],"metadata":{"id":"4iNdPPF3ZTc-","executionInfo":{"status":"ok","timestamp":1687962670100,"user_tz":-540,"elapsed":233963,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### `서브워드토크나이저객체.subwords` : 토큰화된 서브워드 확인"],"metadata":{"id":"NXQojx3yaJRR"}},{"cell_type":"code","source":["print(tokenizer.subwords[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSmR8rCCZm73","executionInfo":{"status":"ok","timestamp":1687962671458,"user_tz":-540,"elapsed":4,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"099bc22b-54eb-4dbf-f00d-69dff389c35a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","#### `서브워드토크나이저객체.encode()` : 입력한 데이터에 대해서 정수 인코딩을 수행한 결과확인\n","\n","<br>\n","\n","* 정수 인코딩을 수행한 결과와 비교"],"metadata":{"id":"y6nWUKZ_aVqb"}},{"cell_type":"code","source":["print(train_df['review'][20], end='\\n\\n')\n","\n","print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WOsXXaf3Z8r0","executionInfo":{"status":"ok","timestamp":1687962671458,"user_tz":-540,"elapsed":3,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"51a4b1b1-5180-497e-8a29-6edf221995d5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n","\n","Tokenized sample question: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","#### `서브워드토크나이저객체.decode()` : 인코딩된 정수에 대하여 문자열로 디코딩\n","\n","<br>\n","\n","* 임의로 선택한 짧은 문장에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩"],"metadata":{"id":"dgUkwpdwadiM"}},{"cell_type":"code","source":["sample_string = \"It's mind-blowing to me that this film was even made.\"\n","\n","# 인코딩한 결과\n","tokenized_string = tokenizer.encode(sample_string)\n","print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n","\n","# 디코딩\n","original_string = tokenizer.decode(tokenized_string)\n","print ('기존 문장 : {}'.format(original_string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGXBw1Sqa6TQ","executionInfo":{"status":"ok","timestamp":1687962802531,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"70671cbf-1c27-4695-8ede-ef048b3c8072"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n","기존 문장 : It's mind-blowing to me that this film was even made.\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","#### `서브워드토크나이저객체.vocab_size` : 단어 집합의 크기를 확인"],"metadata":{"id":"cQoBDoolbGoH"}},{"cell_type":"code","source":["print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lA5PEzGKbA7P","executionInfo":{"status":"ok","timestamp":1687962832670,"user_tz":-540,"elapsed":3,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"b730bb05-7f4c-4f56-bf97-31e465a80310"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합의 크기(Vocab size) : 8268\n"]}]},{"cell_type":"code","source":["for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCax1WjebMoy","executionInfo":{"status":"ok","timestamp":1687962837667,"user_tz":-540,"elapsed":3,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"71de4c69-f6c3-47a6-cefc-9112900400f3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["137 ----> It\n","8051 ----> '\n","8 ----> s \n","910 ----> mind\n","8057 ----> -\n","2169 ----> blow\n","36 ----> ing \n","7 ----> to \n","103 ----> me \n","13 ----> that \n","14 ----> this \n","32 ----> film \n","18 ----> was \n","79 ----> even \n","681 ----> made\n","8058 ----> .\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","* 기존 예제 문장 중 even이라는 단어에 임의로 xyz라는 3개의 글자를 추가\n","- **현재 토크나이저가 even이라는 단어를 이미 하나의 서브워드로 인식하고 있는 상황에서 나머지 xyz를 어떻게 분리하는지를 확인**"],"metadata":{"id":"9hiqMOeeba-E"}},{"cell_type":"code","source":["# 앞의 sample_string에 even 뒤에 임의로 xyz 추가\n","sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n","\n","# 인코딩\n","tokenized_string = tokenizer.encode(sample_string)\n","print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n","\n","# 디코딩\n","original_string = tokenizer.decode(tokenized_string)\n","print ('기존 문장 : {}'.format(original_string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLg-i3ivbN6M","executionInfo":{"status":"ok","timestamp":1687962928895,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"c63b22eb-7389-459f-f0c1-3670aa9313b4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n","기존 문장 : It's mind-blowing to me that this film was evenxyz made.\n"]}]},{"cell_type":"markdown","source":["* **evenxyz에서 even을 독립적으로 분리하고,**\n","\n","  **xyz는 훈련 데이터에서 하나의 단어로서 등장한 적이 없으므로 각각 전부 분리**"],"metadata":{"id":"crN-f0tFboG0"}},{"cell_type":"code","source":["for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgA7UxN0bkHt","executionInfo":{"status":"ok","timestamp":1687962934174,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"1e7edec7-6e13-4e86-a616-9f5273ebc9f6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["137 ----> It\n","8051 ----> '\n","8 ----> s \n","910 ----> mind\n","8057 ----> -\n","2169 ----> blow\n","36 ----> ing \n","7 ----> to \n","103 ----> me \n","13 ----> that \n","14 ----> this \n","32 ----> film \n","18 ----> was \n","7974 ----> even\n","8132 ----> x\n","8133 ----> y\n","997 ----> z \n","681 ----> made\n","8058 ----> .\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","### 네이버 영화 리뷰 토큰화"],"metadata":{"id":"By6g947rbsMS"}},{"cell_type":"code","source":["import pandas as pd\n","import urllib.request\n","import tensorflow_datasets as tfds"],"metadata":{"id":"T7Ow4zCAblZa","executionInfo":{"status":"ok","timestamp":1687962971169,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","train_data = pd.read_table('ratings_train.txt')"],"metadata":{"id":"BtFAMnPvbuNc","executionInfo":{"status":"ok","timestamp":1687962975455,"user_tz":-540,"elapsed":1562,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["print(train_data.isnull().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtExyN0TbvAJ","executionInfo":{"status":"ok","timestamp":1687962977434,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"2f9998db-c023-43b9-dcc5-9ac5ff2e3022"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["id          0\n","document    5\n","label       0\n","dtype: int64\n"]}]},{"cell_type":"code","source":["train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n","print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RB3Ui01Ebv-F","executionInfo":{"status":"ok","timestamp":1687962980777,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"b2de19c2-17da-4030-b4c3-a24c3a23eaf3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"code","source":["tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_data['document'], target_vocab_size=2**13)"],"metadata":{"id":"rAw6Msiibwy_","executionInfo":{"status":"ok","timestamp":1687963324171,"user_tz":-540,"elapsed":323252,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["<br>"],"metadata":{"id":"tnN2IIUDb5B1"}},{"cell_type":"code","source":["print(train_data['document'][20])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1wYw9APbyGs","executionInfo":{"status":"ok","timestamp":1687963324172,"user_tz":-540,"elapsed":17,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"c8348531-45a1-4c49-a9ae-f308e468d5ba"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님\n"]}]},{"cell_type":"code","source":["print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhH08LTfb4XZ","executionInfo":{"status":"ok","timestamp":1687963324172,"user_tz":-540,"elapsed":7,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"8402ae6d-0d19-45cb-f698-5a008e1ef3d0"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"]}]},{"cell_type":"markdown","source":["<br>\n"],"metadata":{"id":"wQt9H__pb6vO"}},{"cell_type":"code","source":["sample_string = train_data['document'][21]\n","\n","# 인코딩\n","tokenized_string = tokenizer.encode(sample_string)\n","print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n","\n","# 디코딩\n","original_string = tokenizer.decode(tokenized_string)\n","print ('기존 문장 : {}'.format(original_string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NrDVqPIpb8hE","executionInfo":{"status":"ok","timestamp":1687963324173,"user_tz":-540,"elapsed":6,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"a420ea2d-835e-4914-8dc0-02453be1088a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인코딩 후의 문장 : [570, 892, 36, 584, 159, 7091, 201]\n","기존 문장 : 보면서 웃지 않는 건 불가능하다\n"]}]},{"cell_type":"code","source":["for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GBvC5Ngb-FR","executionInfo":{"status":"ok","timestamp":1687963324173,"user_tz":-540,"elapsed":5,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"1e67ae80-e7ab-4081-fc8b-a5fcbdbcaa44"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["570 ----> 보면서 \n","892 ----> 웃\n","36 ----> 지 \n","584 ----> 않는 \n","159 ----> 건 \n","7091 ----> 불가능\n","201 ----> 하다\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","<br>\n","\n","## 허깅페이스 토크나이저(Huggingface Tokenizer)\n","- 자연어 처리 스타트업 허깅페이스가 개발한 패키지 tokenizers는 자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공\n","\n","<br>\n","\n","```python\n","pip install tokenizers\n","```"],"metadata":{"id":"lpxYulsUddn4"}},{"cell_type":"markdown","source":["<br>\n","\n","### BERT의 워드피스 토크나이저(BertWordPieceTokenizer)\n","- 구글이 공개한 딥 러닝 모델 BERT에는 WordPiece Tokenizer가 사용\n","- 허깅페이스는 해당 토크나이저를 직접 구현하여 tokenizers라는 패키지를 통해 버트워드피스토크나이저(BertWordPieceTokenizer)를 제공"],"metadata":{"id":"TumFbUm0doqR"}},{"cell_type":"markdown","source":["<br>\n","\n","#### 데이터 로드"],"metadata":{"id":"8Q3Wn1DNdv8z"}},{"cell_type":"code","source":["import pandas as pd\n","import urllib.request\n","from tokenizers import BertWordPieceTokenizer\n","\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eim89_NKdmxB","executionInfo":{"status":"ok","timestamp":1687963515555,"user_tz":-540,"elapsed":1222,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"3b6030fa-ab6c-4b49-cee9-6f2e05fd38b3"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ratings.txt', <http.client.HTTPMessage at 0x7f4eb680ffd0>)"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["#### 데이터 전처리\n","- ratings.txt라는 파일을 데이터프레임으로 로드한 후, 결측값을 제거\n","- naver_review.txt라는 파일로 저장"],"metadata":{"id":"nfn6jO5Ad1a4"}},{"cell_type":"code","source":["naver_df = pd.read_table('ratings.txt')\n","naver_df = naver_df.dropna(how='any')\n","\n","with open('naver_review.txt', 'w', encoding='utf8') as f:\n","    f.write('\\n'.join(naver_df['document']))"],"metadata":{"id":"xPkOsrY9dy1m","executionInfo":{"status":"ok","timestamp":1687963559812,"user_tz":-540,"elapsed":1173,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### BERT 워드피스 토크나이저 설정\n","\n","<br>\n","\n","#### `tokenizers.BertWordPieceTokenizer(lowercase, strip_accents)` : BERT 워드피스 토크나이저 객체 생성\n","- `lowercase` : 대소문자를 구분 여부. True일 경우 구분하지 않음\n","- `strip_accents` : True일 경우 악센트 제거.\n","  - ex) é → e, ô → o\n"],"metadata":{"id":"McQj942rd-8w"}},{"cell_type":"code","source":["tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"],"metadata":{"id":"sRPpBtMdd94X","executionInfo":{"status":"ok","timestamp":1687963645288,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### BERT 워드피스 토크나이저 학습\n","\n","<br>\n","\n","#### `'BERT 워드피스 토크나이저 객체'.train(files, vocab_size, limit_alphabet, min_frequency)`\n","- `files` : 단어 집합을 얻기 위해 학습할 데이터\n","- `vocab_size` : 단어 집합의 크기\n","- `limit_alphabet` : 병합 전의 초기 토큰의 허용 개수\n","- `min_frequency` : 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상"],"metadata":{"id":"VKRfdKEpeWly"}},{"cell_type":"code","source":["data_file = 'naver_review.txt'\n","vocab_size = 30000\n","limit_alphabet = 6000\n","min_frequency = 5"],"metadata":{"id":"_steyfH-eRfu","executionInfo":{"status":"ok","timestamp":1687963652717,"user_tz":-540,"elapsed":1,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["tokenizer.train(files=data_file,\n","                vocab_size=vocab_size,\n","                limit_alphabet=limit_alphabet,\n","                min_frequency=min_frequency)"],"metadata":{"id":"eTye7-69eU7s","executionInfo":{"status":"ok","timestamp":1687963687515,"user_tz":-540,"elapsed":12045,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","- vocab 저장"],"metadata":{"id":"zgvcmqA-etjG"}},{"cell_type":"code","source":["# vocab 저장\n","tokenizer.save_model('./')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjDZth6Neaig","executionInfo":{"status":"ok","timestamp":1687963761509,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"5bdab219-9b58-4f16-9bca-c352b6d8d180"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./vocab.txt']"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["- vocab을 데이터프레임으로 로드"],"metadata":{"id":"G0lysO36ewtT"}},{"cell_type":"code","source":["# vocab 로드\n","df = pd.read_fwf('vocab.txt', header=None)\n","print(df.shape)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"UpoC1vGCevbL","executionInfo":{"status":"ok","timestamp":1687963786412,"user_tz":-540,"elapsed":988,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"69a6ca9d-eaac-44ec-b07f-3edefd782d36"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["(30000, 1)\n"]},{"output_type":"execute_result","data":{"text/plain":["        0\n","0   [PAD]\n","1   [UNK]\n","2   [CLS]\n","3   [SEP]\n","4  [MASK]"],"text/html":["\n","  <div id=\"df-d73c7a5b-723a-44fa-8313-62cef4b43134\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[PAD]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[UNK]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[CLS]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[SEP]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[MASK]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d73c7a5b-723a-44fa-8313-62cef4b43134')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d73c7a5b-723a-44fa-8313-62cef4b43134 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d73c7a5b-723a-44fa-8313-62cef4b43134');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["<br>\n","\n","#### `'BERT 워드피스 토크나이저 객체'.encode(문자열)` : 토큰화\n","* `.ids`는 실질적인 딥 러닝 모델의 입력으로 사용되는 정수 인코딩 결과를 출력\n","* `.tokens`는 해당 토크나이저가 어떻게 토큰화를 진행했는지를 출력\n","* `decode(.ids)`는 정수 시퀀스를 문자열로 복원"],"metadata":{"id":"p5FH_bz1fF6M"}},{"cell_type":"code","source":["encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\n","print('토큰화 결과 :',encoded.tokens)\n","print('정수 인코딩 :',encoded.ids)\n","print('디코딩 :',tokenizer.decode(encoded.ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxYXKwpreyen","executionInfo":{"status":"ok","timestamp":1687963850483,"user_tz":-540,"elapsed":506,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"9c262679-4395-4e8e-c483-b79619da2fb0"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\n","정수 인코딩 : [2111, 20630, 3862, 3573, 24680, 7871, 7378]\n","디코딩 : 아 배고픈데 짜장면먹고싶다\n"]}]},{"cell_type":"code","source":["encoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\n","print('토큰화 결과 :',encoded.tokens)\n","print('정수 인코딩 :',encoded.ids)\n","print('디코딩 :',tokenizer.decode(encoded.ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AX6kFwkSfFKd","executionInfo":{"status":"ok","timestamp":1687963956266,"user_tz":-540,"elapsed":575,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"1a06968f-bd9f-445d-b432-b941d71b7b1d"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["토큰화 결과 : ['커피', '한잔', '##의', '여유', '##를', '즐기', '##다']\n","정수 인코딩 : [12825, 25642, 3242, 12696, 3416, 10784, 3258]\n","디코딩 : 커피 한잔의 여유를 즐기다\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","### 기타 토크나이저\n","* 이 외 **`ByteLevelBPETokenizer`**, **`CharBPETokenizer`**, **`SentencePieceBPETokenizer`** 등이 존재하며 선택에 따라서 사용\n","\n","<br>\n","\n","* **`BertWordPieceTokenizer`** : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n","* **`CharBPETokenizer`** : 오리지널 BPE\n","* **`ByteLevelBPETokenizer`** : BPE의 바이트 레벨 버전\n","* **`SentencePieceBPETokenizer`** : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"],"metadata":{"id":"qAt1bhtKfgsp"}},{"cell_type":"code","source":["from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer"],"metadata":{"id":"l4hypnK_fo_B","executionInfo":{"status":"ok","timestamp":1687963998489,"user_tz":-540,"elapsed":1089,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["tokenizer = SentencePieceBPETokenizer()\n","tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n","\n","encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n","print(encoded.tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHZyIXuefe-V","executionInfo":{"status":"ok","timestamp":1687964020087,"user_tz":-540,"elapsed":19733,"user":{"displayName":"Chansol Lee","userId":"05241681837433185221"}},"outputId":"16033542-675d-47f4-fa43-d064339d1f6b"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["['▁이', '▁영화는', '▁정말', '▁재미있', '습니다.']\n"]}]}]}