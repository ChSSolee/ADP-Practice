---
title: "ADP06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 텍스트 마이닝
- 감성분석, 워드 클라우드
- 이 정보를 클러스터링, 분류, SNA분석에 활용

##### 데이터 전처리
- Corpus : 데이터마이닝의 절차 중 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계, 더이상 추가적인 절차 없이 데이터마이닝 알고리즘 실험에 활용될 수 있는 상태
- VCorpus : 문서를 Corpus class로 만들어주는 함수, R메모리에서만 유지

```{r message=FALSE, warning=FALSE}
library(tm)
```
```{r}
data(crude)
summary(crude)[1:6,]
```
- 문서의 글자 수 확인
```{r}
inspect(crude[1])
```
```{r}
crude[[1]]$content
```

##### 텍스트 데이터를 문서로
VectorSource(text)

##### 문서로 완성된 데이터를 Corpus로
VCorpus(data)

##### 데이터 전처리
- Corpus로 변환된 데이터를 tm_map 함수를 활용하여 텍스트 전처리 수행

tm_map(x, FUN, ...)
- FUN : (tolower : 소문자 / stemDocument : 어근만 남기기 / stripWhitespace : 공백 제거 / removePunctuation : 문장부호 제거 / removeNumbers : 숫자 제거 / removeWords, "word" : "word"라는 단어 제거 / removeWords, stopwords("english") : 불용어 제거 / PlainTextdocument : TextDocument로 변환)

```{r}
news <- readLines("C:/Users/이찬솔/Desktop/숭실/R/ADP/PART 06 실습용 데이터/키워드_뉴스.txt")
```
```{r}
news.corpus <- VCorpus(VectorSource(news))
news.corpus[[1]]$content
```
- 데이터 전처리 (숫자제거, 문장부호 제거, 공백 제거)
```{r}
clean_txt <- function(txt) {
  txt <- tm_map(txt, removeNumbers)
  txt <- tm_map(txt, removePunctuation)
  txt <- tm_map(txt, stripWhitespace)
  return(txt)
}
clean.news <- clean_txt(news.corpus)
clean.news[[1]]$content
```
- 특수문자 제거
```{r}
news_pre <- gsub("[\r\n\t]", ' ', news) # 이스케이프 제거
news_pre <- gsub('[[:punct:]]', ' ', news_pre) # 문장부호 제거
news_pre <- gsub('[[:cntrl:]]', ' ', news_pre) # 특수문자 제거
news_pre <- gsub('\\d+', ' ', news_pre)   # 숫자 제거  
news_pre <- gsub('[a-z]+', ' ', news_pre) # 영 대문자 제거
news_pre <- gsub('[A-Z]+', ' ', news_pre) # 영 소문자 제거
news_pre <- gsub('\\s+', ' ', news_pre) # 2개 이상 공백 교체 
news_pre[[1]]
```

##### 자연어 처리
- 형태소 분석, 문장의 품사만 구분하여 분석에 필요한 품사만 추출

##### KoNLP
- KoNLP가 대표적이며, KoNLP 사용 시 rJava패키지를 설치해야 함
- 자연어 처리에 앞서 reference를 삼을 사전을 설정. 주로 SejongDic 사용, 
- extraNoun, mergeUserDic, SimplePos09 등의 함수로 자연어 처리가 가능

##### 사전에 단어 추가
buildDictionary(ext_dic, data)
- ext_dic : 단어를 추가하고자 하는 사전을 선택 ("woorimalsam" , "sejong", "insighter"s)
- data : 추가하고자 하는 단어와 품사가 들어간 data.frame 또는 txt파일

useSejongDic()

##### 문장에서 명사만 추출 
extraNoun(text)

##### 문장을 형태소로 분리하여 22개의 품사 태그를 달기
SimplePos22(text)

```{r message=FALSE, warning=FALSE}
library(KoNLP)
```
```{r}
useSejongDic()
```
```{r}
sentence <- "아버지가 방에 스르륵 들어가신다"
extractNoun(sentence)
```
- "아버지", "방"은 명사
- "스르륵"은 부사

- sejong사전에 "스르륵"을 부사(mag)로 추가
```{r}
buildDictionary(ext_dic = "sejong", user_dic = data.frame(c("스르륵"), c("mag")))
```
```{r}
extractNoun(sentence)
```
```{r}
knitr::include_graphics("C:/Users/이찬솔/Desktop/숭실/R/ADP/konlp_tags.png")
```

- S : 기호 (sp : 쉼표 / sf : 마침표 / sl : 여는 따음표 및 묶음표 / sr : 닫는 따음표 및 묶음표 / sd : 이음표 / se : 줄임표 / su : 단위기호 / sy : 기타기호) 
- F : 외국어 (f : 외국어)
- N : 체언 (NC : 보통명사 / NQ : 고유명사 / NB : 의존명사 / NP : 대명사 / NN : 수사)
- P : 용언 (PV : 동사 / PA : 형용사 / PX : 보조용언)
- M : 수식언 (MM : 관형사 / MA : 부사)
- I : 독립언 (II : 감탄사)
- J : 관계언 (JC : 격조사 / JX : 보조사 / JP : 서술격조사)
- E : 어미 (EP : 선어말어미 / EC : 연결어미 / ET : 전성어미 / EF : 종결어미)
- X : 접사 (XP : 접두사 / XS : 접미사)
```{r}
SimplePos22(sentence)
```


```{r}
clean_txt2<-function(txt){
  txt <- removeNumbers(txt)  #숫자 제거
  txt <- removePunctuation(txt) #문장부호 제거
  txt <- stripWhitespace(txt) #공백제거
  txt <- gsub("[^[:alnum:]]"," ",txt) #영숫자, 문자를 제외한 것들을 " "으로 처리
  return(txt)
}
```
```{r}
clean.news2<-clean_txt2(news)
Noun.news <- extractNoun(clean.news2) #각 문서마다 명사가 추출
Noun.news[[5]][1:100]
```
```{r}
buildDictionary(ext_dic = "sejong", user_dic = data.frame(c(read.table("C:/Users/이찬솔/Desktop/숭실/R/ADP/PART 06 실습용 데이터/food.txt"))))
extractNoun(clean.news2[[5]])[1:100]
```

```{r}
readLines("C:/Users/이찬솔/Desktop/숭실/R/ADP/PART 06 실습용 데이터/food.txt")
```

- SimplePos22를 활용해 형용사 추출하기.(stringr 패키지의 str_match함수로 형용사 뽑아내기)
```{r message=FALSE, warning=FALSE}
library(stringr)
```
- SimplePos22를 통해 품사를 구분하고, 분석에 용이하게 하기 위해
```{r}
(doc1 <- paste(SimplePos22(clean.news2[[2]])))[1:100]
```
- 품사중 PA가 형용사이므로 형용사만 뽑아내기 위해 str_match함수 이용
```{r}
(doc2<-str_match(doc1,"([가-힣]+)/PA"))[1:100]
```

##### Stemming 
- 어간 추출은 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업
- 즉, 공통 어간을 가지는 단어를 묶는 작업을 Stemming

##### 공통으로 들어가지 않은 부분을 제외
stemDocument(text)
```{r}
library(SnowballC)
(test <- tm::stemDocument(c('analyze', 'analyzed','analyzing')))
```

##### 가장 기본적인 어휘로 완성
stemCompletion(text, dictionary)
- text : stemming이 완료된 문자
- dictionary : 어간 추출이 필요한 단어를 사전에 추가
```{r}
(completion<-stemCompletion(test,dictionary = c('analyze', 'analyzed','analyzing')))
```

#### Term Document Matrix
- 전처리된 데이터에서 각 문서와 단어 간의 사용여부를 이용해 만들어진 matrix
- 문서마다 등장한 단어의 빈도수를 쉽게 파악

TermDocumentMatrix(data, control)
- data : Corpus 형태의 데이터
- control : 사전 변경, 가중치 부여 등의 옵션

```{r}
VC.news <- VCorpus(VectorSource(clean.news2))
VC.news[[1]]$content
```
```{r}
TDM.news <- TermDocumentMatrix(VC.news)
dim(TDM.news)
```
- 10개의 기사에서 1011개의 단어가 추출

```{r}
inspect(TDM.news[1:5,])
```
- TDM구축 결과

##### 명사만 추출하는 TermDocumentMatrix를 만들기위한 사용자 정의함수 생성
```{r}
words <- function(doc){
  doc <- as.character(doc)
  extractNoun(doc)
}
TDM.news2<-TermDocumentMatrix(VC.news, control=list(tokenize=words))
dim(TDM.news2) 
```
- 10개의 기사에서 총 289개의 단어가 추출되어 289개의 행과 10개의 열을 가지는 형태

```{r}
inspect(TDM.news2)
```
- 생성된 TDM에서 전체 문서와 단어의 분포를 확인.

```{r warning=FALSE}
tdm2 <- as.matrix(TDM.news2)
tdm3 <- rowSums(tdm2)
tdm4 <- tdm3[order(tdm3, decreasing = T)]
tdm4[1:10]
```

- 분석에 사용하고자하는 단어들을 별도 사전으로 정의해 해당 단어들에 대해서만 결과를 산출
```{r}
mydict <- c("빅데이터", "스마트", "산업혁명", "인공지능", "사물인터넷", "AI", "스타트업", "머신러닝")
my.news <- TermDocumentMatrix(VC.news, control = list(tokenize = words, dictionary = mydict))
inspect(my.news) 
```

##### TDM을 활용한 분석 및 시각화
- 연관성 분석 : 작성된 TDM에서 특정 단어와의 연관성에 따라 단어 조회

findAssocs(data, terms, corlimit)
- data : TDM 형태의 데이터
- terms : 연관성을 확인할 단어
- corlimint : 최소 연관성

```{r}
words <- function(doc) {
  doc <- as.character(doc)
  extractNoun(doc)
}

TDM.news2 <- TermDocumentMatrix(VC.news, control = list(tokenize = words))

findAssocs(TDM.news2,'빅데이터', 0.9)
```
- 빅데이터란 단어와 무슨 내용이 연관되어 함께 언급되는지 알수 있음

##### 워드 클라우드
wordcloud(words, freq, min.freq, random.order, colors, ...)
- freq : 단어의 빈도
- min.freq : 시각화하려는 단어의 최소 빈도
- random.order : 단어의 배치를 랜덤으로 할지 정함 (F : 빈도순)
- colors : 빈도에 따라 단어의 색을 지정

```{r message=FALSE, warning=FALSE}
library(wordcloud)
```
```{r}
tdm2 <- as.matrix(TDM.news2)
term.freq <- sort(rowSums(tdm2), decreasing = T) 
head(term.freq, 15)
```
```{r}
wordcloud(words = names(term.freq),        # term.freq의 이름만 가져옴.
          freq = term.freq,                # 빈도는 위에 저장한 term.freq.
          min.freq = 5,                    # 최소빈도는 5
          random.order = F,
          colors = brewer.pal(8,'Dark2'))
```