{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83ff6e7",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "# \n",
    "\n",
    "## BeautifulSoup 특징\n",
    "- HTML과 XML 파일에서 데이터를 뽑아내기 위한 파이썬 라이브러릴\n",
    "- HTML과 XML의 트리 구조를 탐색, 검색, 변경 가능\n",
    "- 다양한 파서 (parser)를 선택하여 이용 가능\n",
    "\n",
    "| **파서 (parser)** | **선언** | **장점** | **단점** |\n",
    "| -- | -- | -- | -- |\n",
    "| html.parser | ```BeautifulSoup(markup, 'html.parser')``` | 설치 필요 없음<br/> 적절한 속도 |  |\n",
    "| lxml HTML parser | ```BeautifulSoup(markup, 'lxml')``` | 매우 빠름 | lxml 추가 설치 필요 |\n",
    "| lxml XML parser | ```BeautifulSoup(markup, 'lxml-xml')``` <br/> ```BeautifulSoup(markup, 'xml')``` | 매우 빠름<br/> 유일한 xml parser | lxml 추가 설치 필요 |\n",
    "| html5lib | ```BeautifulSoup(markup, 'html5lib')``` | 웹 브라우저와 같은 방식으로 파싱<br/> 유용한 HTML5 생성 | html5lib 추가 설치 필요 <br/>매우 느림 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92338c",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "## HTML 파싱 (Parsing)\n",
    "### ```BeautifulSoup(markup, 'html.parser')```\n",
    "\n",
    "# \n",
    "\n",
    "### 웹 페이지 예제 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be67396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Page Title</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Heading 1</h1>\n",
    "        <p>Paragraph</p>\n",
    "        <div>\n",
    "            <a href=\"https://www.google.com\">google</a>\n",
    "        </div>\n",
    "        \n",
    "        <div class='class1'>\n",
    "            <a href=\"https://www.naver.com\">naver</a>\n",
    "            <p>b</p>\n",
    "            <p>c</p>\n",
    "        </div>\n",
    "        \n",
    "        <div id=\"id1\">\n",
    "            Example page\n",
    "            <p>g</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7066c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c7316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.html') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4c5d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<title>Page Title</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>Heading 1</h1>\n",
       "<p>Paragraph</p>\n",
       "<div>\n",
       "<a href=\"https://www.google.com\">google</a>\n",
       "</div>\n",
       "<div class=\"class1\">\n",
       "<a href=\"https://www.naver.com\">naver</a>\n",
       "<p>b</p>\n",
       "<p>c</p>\n",
       "</div>\n",
       "<div id=\"id1\">\n",
       "            Example page\n",
       "            <p>g</p>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup\n",
    "# soup.prettify() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a0abd",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "### HTML 태그 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0f8693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Page Title</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d8741dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('title', 'Page Title')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.name, soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "329e23f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<head>\n",
       " <title>Page Title</title>\n",
       " </head>,\n",
       " 'head')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2acf671f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'head'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48928b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Heading 1</h1>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcd9ad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('h1', 'Heading 1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h1.name, soup.h1.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0962d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div>\n",
       "<a href=\"https://www.google.com\">google</a>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe02318",
   "metadata": {},
   "source": [
    "# \n",
    "### HTML 태그 검색\n",
    "### ```.find(name, attrs, ...)``` : 해당 조건에 맞는 하나의 태그를 가져옴\n",
    "- ```name``` : 조건\n",
    "- ```attrs``` : 속성 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1a84ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div>\n",
       "<a href=\"https://www.google.com\">google</a>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_find = soup.find(\"div\")\n",
    "soup_find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a7877",
   "metadata": {},
   "source": [
    "- **하이퍼 레퍼런스 값만 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef51b8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d0422",
   "metadata": {},
   "source": [
    "- **텍스트 값만 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33db188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ceb1e3",
   "metadata": {},
   "source": [
    "# \n",
    "### ```.find_all(name, attrs, ...)``` : 해당 조건에 맞는 모든 태그를 리스트 형태로 가져옴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48f9ae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<div>\n",
       "  <a href=\"https://www.google.com\">google</a>\n",
       "  </div>,\n",
       "  <div class=\"class1\">\n",
       "  <a href=\"https://www.naver.com\">naver</a>\n",
       "  <p>b</p>\n",
       "  <p>c</p>\n",
       "  </div>,\n",
       "  <div id=\"id1\">\n",
       "              Example page\n",
       "              <p>g</p>\n",
       "  </div>],\n",
       " 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_find_all = soup.find_all('div')\n",
    "soup_find_all, len(soup_find_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a3862",
   "metadata": {},
   "source": [
    "- **id 기준으로 탐색**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "899e06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_by_id = soup.find_all(name='div', attrs={'id': 'id1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b5323cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"id1\">\n",
       "             Example page\n",
       "             <p>g</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_by_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181805f7",
   "metadata": {},
   "source": [
    "- **class 기준으로 탐색**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2493b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_by_class = soup.find_all(name='div', attrs={'class': 'class1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14949511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"class1\">\n",
       " <a href=\"https://www.naver.com\">naver</a>\n",
       " <p>b</p>\n",
       " <p>c</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fed00c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b5f2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "436c9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com\n",
      "https://www.naver.com\n"
     ]
    }
   ],
   "source": [
    "for name in site_names:\n",
    "    print(name.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db9491fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google\n",
      "naver\n"
     ]
    }
   ],
   "source": [
    "for name in site_names:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc669a",
   "metadata": {},
   "source": [
    "# \n",
    "### ```.select()``` : CSS 선택자와 같은 형식, 조건에 맞는 모든 태그를 리스트 형태로 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e92d6c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"id1\">\n",
       "             Example page\n",
       "             <p>g</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id1 = soup.select('div#id1')\n",
    "id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78065fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"class1\">\n",
       " <a href=\"https://www.naver.com\">naver</a>\n",
       " <p>b</p>\n",
       " <p>c</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1 = soup.select('div.class1')\n",
    "class1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f930b4c",
   "metadata": {},
   "source": [
    "- **특정 태그만 select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f737afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.naver.com\">naver</a>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1_a = soup.select('div.class1 > a')\n",
    "class1_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477802a",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "## 웹 페이지 콘텐츠 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1204e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting anthem.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile anthem.html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "    <div>\n",
    "        <p id=\"title\">애국가</p>\n",
    "        <p id=\"content\">\n",
    "            동해물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세.<br />\n",
    "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br />\n",
    "        </p>\n",
    "        \n",
    "        <p id=\"content\">\n",
    "            남산 위에 저 소나무, 철갑을 두른 듯 바람 서리 불변함은 우리 기상일세.<br />\n",
    "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br />\n",
    "        </p>\n",
    "        \n",
    "        <p id=\"content\">\n",
    "            가을하늘 공활한데 높고 구름 없이 밝은 달은 우리 가슴 일편단심일세.<br />\n",
    "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br />\n",
    "        </p>\n",
    "        \n",
    "        <p id=\"content\">\n",
    "            이 기상과 이 맘으로 충성을 다하여 괴로우나 즐거우나 나라 사랑하세.<br />\n",
    "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br />\n",
    "        </p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db5f4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7c9a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open('anthem.html', encoding='utf-8')) as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b676cf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "<p id=\"title\">애국가</p>\n",
       "<p id=\"content\">\n",
       "            동해물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세.<br/>\n",
       "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br/>\n",
       "</p>\n",
       "<p id=\"content\">\n",
       "            남산 위에 저 소나무, 철갑을 두른 듯 바람 서리 불변함은 우리 기상일세.<br/>\n",
       "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br/>\n",
       "</p>\n",
       "<p id=\"content\">\n",
       "            가을하늘 공활한데 높고 구름 없이 밝은 달은 우리 가슴 일편단심일세.<br/>\n",
       "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br/>\n",
       "</p>\n",
       "<p id=\"content\">\n",
       "            이 기상과 이 맘으로 충성을 다하여 괴로우나 즐거우나 나라 사랑하세.<br/>\n",
       "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.<br/>\n",
       "</p>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b8293dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('p', {'id': 'title'})\n",
    "contents = soup.find_all('p', {'id': 'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2758a3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'애국가'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1feeaa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            동해물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세.\n",
      "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.\n",
      "\n",
      "\n",
      "            남산 위에 저 소나무, 철갑을 두른 듯 바람 서리 불변함은 우리 기상일세.\n",
      "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.\n",
      "\n",
      "\n",
      "            가을하늘 공활한데 높고 구름 없이 밝은 달은 우리 가슴 일편단심일세.\n",
      "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.\n",
      "\n",
      "\n",
      "            이 기상과 이 맘으로 충성을 다하여 괴로우나 즐거우나 나라 사랑하세.\n",
      "            무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for content in contents:\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d01470",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "## 인터넷 웹 페이지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c0ca94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://suanlab.com\"\n",
    "html = urllib.request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb51c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b37c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = soup.find_all(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "40538d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14] \"메타버스 시대가 오고 있다\"\n",
      "[2020-01-20] \"바이러스 연구부터 뷰티·배달 AI 결합한 비즈니스...\"\n",
      "[2020-10-07] \"이력서 작성·레시피 제공 다양하게 활용되는 GPT3\"\n",
      "[2020-05-20] \"인공지능의 보안 위협\"\n",
      "[2020-03-04] \"데이터 경제 시대\"\n",
      "[2019-12-25] \"마이데이터 시대의 도래 데이터 주권과 새로운 가치\"\n",
      "[2019-09-04] \"농업으로 들어간 인공지능\"\n",
      "[2019-08-07] \"AI시대 지배할 것인가 지배당하며 살 것인가\"\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(label.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "876b25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14] \"메타버스 시대가 오고 있다\"\n",
      "[2020-01-20] \"바이러스 연구부터 뷰티·배달 AI 결합한 비즈니스...\"\n",
      "[2020-10-07] \"이력서 작성·레시피 제공 다양하게 활용되는 GPT3\"\n",
      "[2020-05-20] \"인공지능의 보안 위협\"\n",
      "[2020-03-04] \"데이터 경제 시대\"\n",
      "[2019-12-25] \"마이데이터 시대의 도래 데이터 주권과 새로운 가치\"\n",
      "[2019-09-04] \"농업으로 들어간 인공지능\"\n",
      "[2019-08-07] \"AI시대 지배할 것인가 지배당하며 살 것인가\"\n"
     ]
    }
   ],
   "source": [
    "labels = soup.select('#wrapper > section > div > div > div > div > div > label')\n",
    "\n",
    "for label in labels:\n",
    "    print(label.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330f1be",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "## 네이버 영화 리뷰 스크래핑\n",
    "https://movie.naver.com/movie/point/af/list.naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c910208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7ec4f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://movie.naver.com/movie/point/af/list.naver\"\n",
    "search_url = urllib.request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a1b483bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(search_url, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98190c",
   "metadata": {},
   "source": [
    "# \n",
    "#### 네이버 영화 개별 리뷰 url\n",
    "https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=204624&target=after\n",
    "- \"mcode&sword=\"의 숫자만 변경하면 리뷰에 대한 스크래핑 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d4d88924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_reviews(mcode, page_num=10):\n",
    "    \n",
    "    movie_review_df = pd.DataFrame(columns=('Title', 'Score', 'Review'))\n",
    "    url = \"https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword=\" + \\\n",
    "        str(mcode) + \"&target=after\"\n",
    "    idx = 0\n",
    "    \n",
    "    for _ in range(0, page_num):\n",
    "        movie_page = urllib.request.urlopen(url).read()\n",
    "        movie_page_soup = BeautifulSoup(movie_page, 'html.parser')\n",
    "        \n",
    "        review_list = movie_page_soup.find_all('td', {'class': 'title'})\n",
    "        for review in review_list:\n",
    "            title = review.find('a', {'class': 'movie color_b'}).get_text()\n",
    "            score = review.find('em').get_text()\n",
    "            review_text = review.find_all(\"a\")[1]['onclick']\n",
    "            review_text = review_text.replace(\"report(\", \"\").split(\"', '\")[2]\n",
    "            movie_review_df.loc[idx] = [title, score, review_text]\n",
    "            \n",
    "            idx += 1\n",
    "            print('#', end=\"\")\n",
    "        \n",
    "        # 리뷰의 마지막 페이지 (page_num = 10) 까지 스크래핑\n",
    "        try:\n",
    "            url = \"https://movie.naver.com\" + \\\n",
    "                movie_page_soup.find('a', {'class': 'pg_next'}).get('href')\n",
    "            \n",
    "        # 더이상 페이지가 없으면 종료\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    return movie_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b344227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################"
     ]
    }
   ],
   "source": [
    "movie_review_df = get_movie_reviews(18847, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7bc6de52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3fac6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>타이타닉</td>\n",
       "      <td>10</td>\n",
       "      <td>언제봐도 멋진 연화입니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>타이타닉</td>\n",
       "      <td>10</td>\n",
       "      <td>그냥 짱임 진짜 디카프리오 개잘생김</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>타이타닉</td>\n",
       "      <td>10</td>\n",
       "      <td>내가 봤던 영화중 가장 감동적임</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Title Score               Review\n",
       "0  타이타닉    10        언제봐도 멋진 연화입니다\n",
       "1  타이타닉    10  그냥 짱임 진짜 디카프리오 개잘생김\n",
       "2  타이타닉    10    내가 봤던 영화중 가장 감동적임"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5e13b",
   "metadata": {},
   "source": [
    "# \n",
    "### 현재 상영작 리뷰 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "05386919",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://movie.naver.com/movie/point/af/list.naver\"\n",
    "\n",
    "naver_movie = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(naver_movie, 'html.parser')\n",
    "select = soup.find('select', {'id': 'current_movie'})\n",
    "movies = select.find_all('option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fdde6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dict = {}\n",
    "\n",
    "for movie in movies[1:]:\n",
    "    movies_dict[movie.get('value')] = movie.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bc9cde14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################"
     ]
    }
   ],
   "source": [
    "movie_review_df = pd.DataFrame(columns=(\"Title\", \"Score\", \"Review\"))\n",
    "\n",
    "for mcode in movies_dict:\n",
    "    df = get_movie_reviews(mcode, 1)\n",
    "    movie_review_df = pd.concat([movie_review_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "75605818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>극장판 엄마 까투리: 도시로 간 까투리 가족</td>\n",
       "      <td>9</td>\n",
       "      <td>흥미로운 스토리 좋은이야기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>극장판 엄마 까투리: 도시로 간 까투리 가족</td>\n",
       "      <td>10</td>\n",
       "      <td>왜 어른인 내가 눙물이 나지 ㅠ ㅠ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>극장판 엄마 까투리: 도시로 간 까투리 가족</td>\n",
       "      <td>10</td>\n",
       "      <td>스토리도 좋고 너무 좋았어요</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Title Score               Review\n",
       "0  극장판 엄마 까투리: 도시로 간 까투리 가족      9       흥미로운 스토리 좋은이야기\n",
       "1  극장판 엄마 까투리: 도시로 간 까투리 가족     10  왜 어른인 내가 눙물이 나지 ㅠ ㅠ\n",
       "2  극장판 엄마 까투리: 도시로 간 까투리 가족     10      스토리도 좋고 너무 좋았어요"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "01a0f73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1109, 3)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae8aed",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "## Naver 날씨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0ec90fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b9806d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather():\n",
    "    print(\"[오늘의 파주 목동동 날씨]\")\n",
    "    \n",
    "    url = \"https://search.naver.com/search.naver?sm=tab_hty.top&where=nexearch&query=%ED%8C%8C%EC%A3%BC+%EB%AA%A9%EB%8F%99%EB%8F%99%EB%82%A0%EC%94%A8&oquery=%ED%8C%8C%EC%A3%BC+%EC%9A%B4%EC%A0%95+%EB%82%A0%EC%94%A8&tqi=hxJrFlp0Yidss4C64plssssssEC-066605\"\n",
    "    weather_url = urllib.request.urlopen(url).read() \n",
    "    soup = BeautifulSoup(weather_url, 'html.parser')\n",
    "    \n",
    "    # 어제보다 2.6° 높아요  흐림 / 체감 25.3° 습도 78% 바람(서풍) 0.5m/s\n",
    "    today = soup.find('div', attrs={'class': 'temperature_info'}).get_text().lstrip()\n",
    "    print(today)\n",
    "    \n",
    "    # 최저 / 최고 온도\n",
    "    min_temp = soup.find('span', attrs={'class': 'lowest'}).get_text()\n",
    "    max_temp = soup.find('span', attrs={'class': 'highest'}).get_text()\n",
    "    print(min_temp, max_temp)\n",
    "    \n",
    "    # 오전 / 오후 강수확률\n",
    "    rainfall = soup.find_all('span', attrs={'class': 'weather_left'})\n",
    "    morning = rainfall[0].get_text().lstrip()\n",
    "    afternoon = rainfall[1].get_text().lstrip()\n",
    "    print(morning, afternoon)\n",
    "    \n",
    "    # 미세먼지, 초미세먼지, 자외선, 일몰\n",
    "    chart1 = soup.find_all('li', attrs={'class': 'item_today level2'})\n",
    "    dust = chart1[0].get_text().lstrip()\n",
    "    micro_dust = chart1[1].get_text().lstrip()\n",
    "    \n",
    "    chart2 = soup.find_all('li', attrs={'class': 'item_today level3'})\n",
    "    ray = chart2[0].get_text().lstrip()\n",
    "    \n",
    "    sun = soup.find('li', attrs={'class': 'item_today type_sun'})\n",
    "    type_sun = sun.get_text().lstrip()\n",
    "    print(dust, micro_dust, ray, type_sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "51bd16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[오늘의 파주 목동동 날씨]\n",
      "어제보다 2.4° 높아요  흐림   체감 25.0° 습도 77% 바람(서풍) 0.2m/s  \n",
      "최저기온18° 최고기온27°\n",
      "오전 10%  오후 20% \n",
      "미세먼지 좋음   초미세먼지 좋음   자외선 높음   일몰 18:47  \n"
     ]
    }
   ],
   "source": [
    "scrape_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220b0c8",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "## Naver 헤드라인 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8d46841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "2c81e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_headline():\n",
    "    \n",
    "    today_section_df = pd.DataFrame(columns=('Title', 'Link', \"Media\", \"Datetime\", \"Article\"))\n",
    "    idx = 0\n",
    "    \n",
    "    print(\"[오늘의 헤드라인 뉴스]\")\n",
    "    \n",
    "    url = \"https://sports.news.naver.com/index\"\n",
    "    news_url = urllib.request.urlopen(url).read() \n",
    "    soup = BeautifulSoup(news_url, 'html.parser')\n",
    "    \n",
    "    today_section = soup.find('div', {'class': 'today_section type_no_da'}).\\\n",
    "        find('ul', {'class':'today_list'}).\\\n",
    "        find_all('li', {'class': 'today_item'})\n",
    "    \n",
    "    for i in range(0, len(today_section)-1):\n",
    "        \n",
    "        title = today_section[i].find('strong', {'class':'title'}).get_text()\n",
    "        href = today_section[i].find('a', {'class':'link_today'}).get('href')\n",
    "        \n",
    "        url2 = \"https://sports.news.naver.com\" + href\n",
    "        news_url2 = urllib.request.urlopen(url2).read() \n",
    "        soup2 = BeautifulSoup(news_url2, 'html.parser')\n",
    "        \n",
    "        media = soup2.find('span', {'id': 'pressLogo'}).find('a').get('href')\n",
    "        datetime = soup2.find('div', {'class': 'info'}).find('span').get_text()\n",
    "        article = soup2.find('div', {'id': 'newsEndContents'}).get_text()\n",
    "        \n",
    "        today_section_df.loc[idx] = [title, href, media, datetime, article]\n",
    "        \n",
    "        idx += 1\n",
    "    \n",
    "    return today_section_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "9a54f014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[오늘의 헤드라인 뉴스]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Media</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"이강인과 무리키 교체? 그걸로 게임 끝났어! \"…마요르카 지역지 분노</td>\n",
       "      <td>/news?oid=413&amp;aid=0000146364</td>\n",
       "      <td>http://www.interfootball.co.kr</td>\n",
       "      <td>기사입력 2022.09.12. 오후 01:41</td>\n",
       "      <td>\\n마요르카가 레알 마드리드 원정에서 완패를 당했다. 팬들은 이강인과 베다트 무리키...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스페인 '도움 1위'가 '韓 대표팀' 못든다? …하루남은 이강인의 운명</td>\n",
       "      <td>/news?oid=008&amp;aid=0004793554</td>\n",
       "      <td>http://www.mt.co.kr/</td>\n",
       "      <td>기사입력 2022.09.12. 오후 02:45</td>\n",
       "      <td>\\n사진=이강인 인스타그램최근 상승세를 타던 이강인(마요르카)이 마침내 스페인 프리...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'콘테가 틀렸다' 베르바인, 살라 제치고 리버풀 통합 베스트11</td>\n",
       "      <td>/news?oid=076&amp;aid=0003914751</td>\n",
       "      <td>http://sports.chosun.com</td>\n",
       "      <td>기사입력 2022.09.12. 오후 12:28</td>\n",
       "      <td>\\n[스포츠조선 김성원 기자]위기의 리버풀이다. 잉글랜드 프리미어리그(EPL)에선 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"최악의 마무리\" LAD 출신 전설, ATL 가서도 애물단지됐다</td>\n",
       "      <td>/news?oid=108&amp;aid=0003086298</td>\n",
       "      <td>http://star.moneytoday.co.kr</td>\n",
       "      <td>기사입력 2022.09.12. 오후 01:29</td>\n",
       "      <td>\\n애틀랜타의 켄리 잰슨(오른쪽)이 12일(한국시간) 미국 워싱턴주 시애틀 T-모바...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>오지환 부상에 끝난 1년 전 LG처럼…PS 앞두고 한 해 농사 끝날라</td>\n",
       "      <td>/news?oid=109&amp;aid=0004697141</td>\n",
       "      <td>http://www.osen.co.kr</td>\n",
       "      <td>기사입력 2022.09.12. 오후 04:20</td>\n",
       "      <td>\\n  [OSEN=고척, 조은정 기자] KT 박병호가 발목 부상을 입은 뒤 구급차에...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title                          Link  \\\n",
       "0  \"이강인과 무리키 교체? 그걸로 게임 끝났어! \"…마요르카 지역지 분노  /news?oid=413&aid=0000146364   \n",
       "1  스페인 '도움 1위'가 '韓 대표팀' 못든다? …하루남은 이강인의 운명  /news?oid=008&aid=0004793554   \n",
       "2      '콘테가 틀렸다' 베르바인, 살라 제치고 리버풀 통합 베스트11  /news?oid=076&aid=0003914751   \n",
       "3      \"최악의 마무리\" LAD 출신 전설, ATL 가서도 애물단지됐다  /news?oid=108&aid=0003086298   \n",
       "4   오지환 부상에 끝난 1년 전 LG처럼…PS 앞두고 한 해 농사 끝날라  /news?oid=109&aid=0004697141   \n",
       "\n",
       "                            Media                   Datetime  \\\n",
       "0  http://www.interfootball.co.kr  기사입력 2022.09.12. 오후 01:41   \n",
       "1            http://www.mt.co.kr/  기사입력 2022.09.12. 오후 02:45   \n",
       "2        http://sports.chosun.com  기사입력 2022.09.12. 오후 12:28   \n",
       "3    http://star.moneytoday.co.kr  기사입력 2022.09.12. 오후 01:29   \n",
       "4           http://www.osen.co.kr  기사입력 2022.09.12. 오후 04:20   \n",
       "\n",
       "                                             Article  \n",
       "0  \\n마요르카가 레알 마드리드 원정에서 완패를 당했다. 팬들은 이강인과 베다트 무리키...  \n",
       "1  \\n사진=이강인 인스타그램최근 상승세를 타던 이강인(마요르카)이 마침내 스페인 프리...  \n",
       "2  \\n[스포츠조선 김성원 기자]위기의 리버풀이다. 잉글랜드 프리미어리그(EPL)에선 ...  \n",
       "3  \\n애틀랜타의 켄리 잰슨(오른쪽)이 12일(한국시간) 미국 워싱턴주 시애틀 T-모바...  \n",
       "4  \\n  [OSEN=고척, 조은정 기자] KT 박병호가 발목 부상을 입은 뒤 구급차에...  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_headline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904f274",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "## 해커스 영어 명언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "7aede0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_english():\n",
    "    \n",
    "    url = \"https://www.hackers.co.kr/?c=s_eng/eng_contents/B_others_wisesay&keywd=haceng_maincenter_wisesay_190703&logger_kw=haceng_maincenter_wisesay_190703\"    \n",
    "    weather_url = urllib.request.urlopen(url).read() \n",
    "    soup = BeautifulSoup(weather_url, 'html.parser')\n",
    "    text_en = soup.find('div', {'class': 'text_en'}).get_text()\n",
    "    text_ko = soup.find('div', {'class': 'text_ko'}).get_text()\n",
    "    print(\"[오늘의 영어 명언]\")\n",
    "    print(text_en)\n",
    "    print(\"[해석보기]\")\n",
    "    print(text_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "68f13392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[오늘의 영어 명언]\n",
      "\n",
      "We must use time as a tool, not as a crutch.\n",
      "John F. Kennedy\n",
      "\n",
      "[해석보기]\n",
      "\n",
      "우리는 시간을 도구로 사용해야 하며, 시간에 의존해서는 안 된다.\n",
      "존 F. 케네디\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_english()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f086a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
